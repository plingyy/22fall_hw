{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9izhPxmmGHRg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf0ANCiIJZQT"
      },
      "source": [
        "4(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBmI91RxtfD4"
      },
      "outputs": [],
      "source": [
        "text_path = '/content/drive/MyDrive/5460/book-war-and-peace.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7mZh8XK_gOC"
      },
      "outputs": [],
      "source": [
        "f = open(text_path)\n",
        "content = []\n",
        "for line in f:\n",
        "  content.append(line.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxjpg7Qb_yMB"
      },
      "outputs": [],
      "source": [
        "content = ''.join(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqOd8zAqCFQi",
        "outputId": "9743c153-c7a6-451b-bb97-6f26e540e1e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "char_list = sorted(list(set(content)))\n",
        "length = len(char_list)\n",
        "length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vhbrlWoF-iq"
      },
      "outputs": [],
      "source": [
        "token_index = {}\n",
        "char_index = {}\n",
        "for index, char in enumerate(char_list):\n",
        "  token_index[char] = index\n",
        "  char_index[index] = char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIyqxC76FZVD"
      },
      "outputs": [],
      "source": [
        "def vector(text):\n",
        "  vector = np.zeros([len(text), length])\n",
        "  index = np.zeros(len(text))\n",
        "  for i, char in enumerate(text):\n",
        "    vector[i, token_index[char]] = 1\n",
        "    index[i] = token_index[char]\n",
        "  return torch.Tensor(vector), torch.Tensor(index).long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-vt1blSI2xb",
        "outputId": "8798a1ea-6d81-4ff4-e278-433b7538f408"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
              " tensor([27, 32, 25,  ..., 71, 69,  9]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "content_vector, index = vector(content)\n",
        "content_vector, index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAoZXcJdJ-GN"
      },
      "source": [
        "4(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js4P274g8tRM",
        "outputId": "2cbeea47-da1f-4358-b3e1-26e287656e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmfUhNidG_U4",
        "outputId": "0918f70c-0e3f-4429-bac3-de962c97a3a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100430, 25107)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# parameters\n",
        "input_size = length\n",
        "output_size = length\n",
        "hidden_dim = 200\n",
        "num_layers = 1\n",
        "\n",
        "# creat training and validation data set\n",
        "\n",
        "percent = 0.8\n",
        "\n",
        "X_train = content_vector[:int(percent*len(content_vector))]\n",
        "X_val = content_vector[int(percent*len(content_vector)):]\n",
        "y_train = index[:int(percent*len(content_vector))]\n",
        "y_val = index[int(percent*len(content_vector)):]\n",
        "\n",
        "# minus 1 here makes sure y also has the dimension of 25\n",
        "train_batch_num = (len(X_train)-1)//25\n",
        "val_batch_num = (len(X_val)-1)//25\n",
        "\n",
        "def batch_generator(i, x, y):\n",
        "  x_out = x[i*25:(i+1)*25].to(device)\n",
        "  y_out = y[i*25+1:(i+1)*25+1].to(device)\n",
        "  return x_out, y_out\n",
        "\n",
        "train_batch_num, val_batch_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ5rj4DOKdwV"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_dim, num_layers):\n",
        "    super(RNN, self).__init__()\n",
        "\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.rnn = nn.RNN(\n",
        "        input_size = input_size, \n",
        "        hidden_size = hidden_dim,\n",
        "        num_layers = num_layers,\n",
        "        nonlinearity = 'tanh'\n",
        "    )\n",
        "    self.linear1 = nn.Linear(input_size, input_size)\n",
        "    self.linear2 = nn.Linear(hidden_dim, output_size)\n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "    x = self.linear1(x)\n",
        "    x = x.view(-1, 1, input_size)\n",
        "    output, h_n = self.rnn(x, hidden)\n",
        "    output = output.squeeze(1)\n",
        "    output = self.linear2(output)\n",
        "    return output, h_n\n",
        "\n",
        "net = RNN(input_size, output_size, hidden_dim, num_layers).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vAN7hzOOHzF"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "learning_rate = 1e-3\n",
        "batch_size = 1\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uDk_jKgcG1nn",
        "outputId": "d926da40-73c9-49eb-88f8-353a06909ddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1 iteration:  1000 ,average train loss:  2.6318 , val loss:  2.8936\n",
            "Epoch:  1 iteration:  2000 ,average train loss:  2.2962 , val loss:  2.0381\n",
            "Epoch:  1 iteration:  3000 ,average train loss:  2.1399 , val loss:  2.2235\n",
            "Epoch:  1 iteration:  4000 ,average train loss:  2.0904 , val loss:  2.4517\n",
            "Epoch:  1 iteration:  5000 ,average train loss:  1.9992 , val loss:  2.2644\n",
            "Epoch:  1 iteration:  6000 ,average train loss:  2.0145 , val loss:  1.7624\n",
            "Epoch:  1 iteration:  7000 ,average train loss:  1.8947 , val loss:  1.9578\n",
            "Epoch:  1 iteration:  8000 ,average train loss:  1.8159 , val loss:  2.2722\n",
            "Epoch:  1 iteration:  9000 ,average train loss:  1.8646 , val loss:  1.8051\n",
            "Epoch:  1 iteration:  10000 ,average train loss:  1.8868 , val loss:  1.9198\n",
            "Epoch:  1 iteration:  11000 ,average train loss:  1.8305 , val loss:  1.7081\n",
            "Epoch:  1 iteration:  12000 ,average train loss:  1.8115 , val loss:  1.283\n",
            "Epoch:  1 iteration:  13000 ,average train loss:  1.8304 , val loss:  1.9235\n",
            "Epoch:  1 iteration:  14000 ,average train loss:  1.8125 , val loss:  1.724\n",
            "Epoch:  1 iteration:  15000 ,average train loss:  1.7775 , val loss:  1.7819\n",
            "Epoch:  1 iteration:  16000 ,average train loss:  1.8581 , val loss:  1.8708\n",
            "Epoch:  1 iteration:  17000 ,average train loss:  1.7534 , val loss:  2.6494\n",
            "Epoch:  1 iteration:  18000 ,average train loss:  1.709 , val loss:  1.8025\n",
            "Epoch:  1 iteration:  19000 ,average train loss:  1.695 , val loss:  2.4195\n",
            "Epoch:  1 iteration:  20000 ,average train loss:  1.7152 , val loss:  1.68\n",
            "Epoch:  1 iteration:  21000 ,average train loss:  1.7285 , val loss:  1.3873\n",
            "Epoch:  1 iteration:  22000 ,average train loss:  1.6917 , val loss:  1.8663\n",
            "Epoch:  1 iteration:  23000 ,average train loss:  1.6499 , val loss:  1.5544\n",
            "Epoch:  1 iteration:  24000 ,average train loss:  1.7551 , val loss:  1.7649\n",
            "Epoch:  1 iteration:  25000 ,average train loss:  1.7139 , val loss:  1.9466\n",
            "Epoch:  1 iteration:  26000 ,average train loss:  1.7285 , val loss:  2.1334\n",
            "Epoch:  1 iteration:  27000 ,average train loss:  1.7068 , val loss:  1.7147\n",
            "Epoch:  1 iteration:  28000 ,average train loss:  1.6671 , val loss:  1.5639\n",
            "Epoch:  1 iteration:  29000 ,average train loss:  1.6364 , val loss:  1.6035\n",
            "Epoch:  1 iteration:  30000 ,average train loss:  1.7334 , val loss:  1.7174\n",
            "Epoch:  1 iteration:  31000 ,average train loss:  1.771 , val loss:  1.0875\n",
            "Epoch:  1 iteration:  32000 ,average train loss:  1.7036 , val loss:  1.8379\n",
            "Epoch:  1 iteration:  33000 ,average train loss:  1.666 , val loss:  0.9325\n",
            "Epoch:  1 iteration:  34000 ,average train loss:  1.6844 , val loss:  1.8563\n",
            "Epoch:  1 iteration:  35000 ,average train loss:  1.6927 , val loss:  2.2914\n",
            "Epoch:  1 iteration:  36000 ,average train loss:  1.7148 , val loss:  1.7317\n",
            "Epoch:  1 iteration:  37000 ,average train loss:  1.727 , val loss:  1.9356\n",
            "Epoch:  1 iteration:  38000 ,average train loss:  1.7367 , val loss:  1.5781\n",
            "Epoch:  1 iteration:  39000 ,average train loss:  1.6662 , val loss:  1.9152\n",
            "Epoch:  1 iteration:  40000 ,average train loss:  1.697 , val loss:  1.8822\n",
            "Epoch:  1 iteration:  41000 ,average train loss:  1.6962 , val loss:  2.3603\n",
            "Epoch:  1 iteration:  42000 ,average train loss:  1.6389 , val loss:  1.898\n",
            "Epoch:  1 iteration:  43000 ,average train loss:  1.7129 , val loss:  1.4495\n",
            "Epoch:  1 iteration:  44000 ,average train loss:  1.6772 , val loss:  1.7499\n",
            "Epoch:  1 iteration:  45000 ,average train loss:  1.7584 , val loss:  2.0143\n",
            "Epoch:  1 iteration:  46000 ,average train loss:  1.6843 , val loss:  1.8089\n",
            "Epoch:  1 iteration:  47000 ,average train loss:  1.6587 , val loss:  1.8166\n",
            "Epoch:  1 iteration:  48000 ,average train loss:  1.6166 , val loss:  2.3593\n",
            "Epoch:  1 iteration:  49000 ,average train loss:  1.5707 , val loss:  1.8306\n",
            "Epoch:  1 iteration:  50000 ,average train loss:  1.703 , val loss:  2.1648\n",
            "Epoch:  1 iteration:  51000 ,average train loss:  1.7526 , val loss:  1.679\n",
            "Epoch:  1 iteration:  52000 ,average train loss:  1.6665 , val loss:  1.6924\n",
            "Epoch:  1 iteration:  53000 ,average train loss:  1.6995 , val loss:  2.3996\n",
            "Epoch:  1 iteration:  54000 ,average train loss:  1.6523 , val loss:  1.5208\n",
            "Epoch:  1 iteration:  55000 ,average train loss:  1.6459 , val loss:  1.4023\n",
            "Epoch:  1 iteration:  56000 ,average train loss:  1.6938 , val loss:  2.2706\n",
            "Epoch:  1 iteration:  57000 ,average train loss:  1.6593 , val loss:  1.9191\n",
            "Epoch:  1 iteration:  58000 ,average train loss:  1.6075 , val loss:  1.9692\n",
            "Epoch:  1 iteration:  59000 ,average train loss:  1.5857 , val loss:  1.7262\n",
            "Epoch:  1 iteration:  60000 ,average train loss:  1.6082 , val loss:  1.584\n",
            "Epoch:  1 iteration:  61000 ,average train loss:  1.5902 , val loss:  2.4776\n",
            "Epoch:  1 iteration:  62000 ,average train loss:  1.6691 , val loss:  2.1019\n",
            "Epoch:  1 iteration:  63000 ,average train loss:  1.7071 , val loss:  1.4951\n",
            "Epoch:  1 iteration:  64000 ,average train loss:  1.642 , val loss:  1.6884\n",
            "Epoch:  1 iteration:  65000 ,average train loss:  1.6654 , val loss:  1.6894\n",
            "Epoch:  1 iteration:  66000 ,average train loss:  1.659 , val loss:  1.3588\n",
            "Epoch:  1 iteration:  67000 ,average train loss:  1.6972 , val loss:  1.6865\n",
            "Epoch:  1 iteration:  68000 ,average train loss:  1.6513 , val loss:  2.1226\n",
            "Epoch:  1 iteration:  69000 ,average train loss:  1.7745 , val loss:  3.0228\n",
            "Epoch:  1 iteration:  70000 ,average train loss:  1.6601 , val loss:  1.8463\n",
            "Epoch:  1 iteration:  71000 ,average train loss:  1.6278 , val loss:  1.4299\n",
            "Epoch:  1 iteration:  72000 ,average train loss:  1.6817 , val loss:  1.6787\n",
            "Epoch:  1 iteration:  73000 ,average train loss:  1.6905 , val loss:  1.7848\n",
            "Epoch:  1 iteration:  74000 ,average train loss:  1.6181 , val loss:  1.9809\n",
            "Epoch:  1 iteration:  75000 ,average train loss:  1.5894 , val loss:  3.3883\n",
            "Epoch:  1 iteration:  76000 ,average train loss:  1.589 , val loss:  3.1749\n",
            "Epoch:  1 iteration:  77000 ,average train loss:  1.6639 , val loss:  2.5824\n",
            "Epoch:  1 iteration:  78000 ,average train loss:  1.6492 , val loss:  1.7917\n",
            "Epoch:  1 iteration:  79000 ,average train loss:  1.6097 , val loss:  1.8412\n",
            "Epoch:  1 iteration:  80000 ,average train loss:  1.669 , val loss:  1.6291\n",
            "Epoch:  1 iteration:  81000 ,average train loss:  1.6551 , val loss:  1.4425\n",
            "Epoch:  1 iteration:  82000 ,average train loss:  1.6277 , val loss:  1.2375\n",
            "Epoch:  1 iteration:  83000 ,average train loss:  1.5698 , val loss:  1.4484\n",
            "Epoch:  1 iteration:  84000 ,average train loss:  1.6136 , val loss:  2.2654\n",
            "Epoch:  1 iteration:  85000 ,average train loss:  1.656 , val loss:  2.2214\n",
            "Epoch:  1 iteration:  86000 ,average train loss:  1.6293 , val loss:  2.0336\n",
            "Epoch:  1 iteration:  87000 ,average train loss:  1.6821 , val loss:  2.005\n",
            "Epoch:  1 iteration:  88000 ,average train loss:  1.6205 , val loss:  1.5386\n",
            "Epoch:  1 iteration:  89000 ,average train loss:  1.6364 , val loss:  1.3843\n",
            "Epoch:  1 iteration:  90000 ,average train loss:  1.6407 , val loss:  2.2184\n",
            "Epoch:  1 iteration:  91000 ,average train loss:  1.7335 , val loss:  1.5107\n",
            "Epoch:  1 iteration:  92000 ,average train loss:  1.6269 , val loss:  1.5388\n",
            "Epoch:  1 iteration:  93000 ,average train loss:  1.6721 , val loss:  1.947\n",
            "Epoch:  1 iteration:  94000 ,average train loss:  1.6576 , val loss:  1.5571\n",
            "Epoch:  1 iteration:  95000 ,average train loss:  1.5961 , val loss:  1.6854\n",
            "Epoch:  1 iteration:  96000 ,average train loss:  1.6169 , val loss:  2.5367\n",
            "Epoch:  1 iteration:  97000 ,average train loss:  1.6914 , val loss:  1.7941\n",
            "Epoch:  1 iteration:  98000 ,average train loss:  1.6228 , val loss:  2.8433\n",
            "Epoch:  1 iteration:  99000 ,average train loss:  1.5547 , val loss:  1.4075\n",
            "Epoch:  1 iteration:  100000 ,average train loss:  1.6001 , val loss:  1.7252\n",
            "Epoch:  2 iteration:  1000 ,average train loss:  1.7203 , val loss:  1.9796\n",
            "Epoch:  2 iteration:  2000 ,average train loss:  1.6342 , val loss:  1.5089\n",
            "Epoch:  2 iteration:  3000 ,average train loss:  1.6024 , val loss:  1.5387\n",
            "Epoch:  2 iteration:  4000 ,average train loss:  1.6421 , val loss:  1.5256\n",
            "Epoch:  2 iteration:  5000 ,average train loss:  1.623 , val loss:  1.8644\n",
            "Epoch:  2 iteration:  6000 ,average train loss:  1.6929 , val loss:  1.7509\n",
            "Epoch:  2 iteration:  7000 ,average train loss:  1.6108 , val loss:  1.9584\n",
            "Epoch:  2 iteration:  8000 ,average train loss:  1.5704 , val loss:  2.1945\n",
            "Epoch:  2 iteration:  9000 ,average train loss:  1.6421 , val loss:  1.5096\n",
            "Epoch:  2 iteration:  10000 ,average train loss:  1.6679 , val loss:  1.103\n",
            "Epoch:  2 iteration:  11000 ,average train loss:  1.6067 , val loss:  2.005\n",
            "Epoch:  2 iteration:  12000 ,average train loss:  1.6017 , val loss:  1.7181\n",
            "Epoch:  2 iteration:  13000 ,average train loss:  1.6674 , val loss:  2.1615\n",
            "Epoch:  2 iteration:  14000 ,average train loss:  1.6658 , val loss:  1.3673\n",
            "Epoch:  2 iteration:  15000 ,average train loss:  1.625 , val loss:  1.6218\n",
            "Epoch:  2 iteration:  16000 ,average train loss:  1.7228 , val loss:  1.7212\n",
            "Epoch:  2 iteration:  17000 ,average train loss:  1.6337 , val loss:  1.5022\n",
            "Epoch:  2 iteration:  18000 ,average train loss:  1.5874 , val loss:  1.7155\n",
            "Epoch:  2 iteration:  19000 ,average train loss:  1.5858 , val loss:  2.2655\n",
            "Epoch:  2 iteration:  20000 ,average train loss:  1.5986 , val loss:  2.1696\n",
            "Epoch:  2 iteration:  21000 ,average train loss:  1.5999 , val loss:  1.8018\n",
            "Epoch:  2 iteration:  22000 ,average train loss:  1.5811 , val loss:  2.6291\n",
            "Epoch:  2 iteration:  23000 ,average train loss:  1.5526 , val loss:  0.9695\n",
            "Epoch:  2 iteration:  24000 ,average train loss:  1.6397 , val loss:  1.2581\n",
            "Epoch:  2 iteration:  25000 ,average train loss:  1.6116 , val loss:  1.7704\n",
            "Epoch:  2 iteration:  26000 ,average train loss:  1.6337 , val loss:  1.3511\n",
            "Epoch:  2 iteration:  27000 ,average train loss:  1.6213 , val loss:  2.1088\n",
            "Epoch:  2 iteration:  28000 ,average train loss:  1.5913 , val loss:  1.332\n",
            "Epoch:  2 iteration:  29000 ,average train loss:  1.5581 , val loss:  1.7888\n",
            "Epoch:  2 iteration:  30000 ,average train loss:  1.6428 , val loss:  2.3821\n",
            "Epoch:  2 iteration:  31000 ,average train loss:  1.6788 , val loss:  1.3826\n",
            "Epoch:  2 iteration:  32000 ,average train loss:  1.6351 , val loss:  1.6603\n",
            "Epoch:  2 iteration:  33000 ,average train loss:  1.6022 , val loss:  1.834\n",
            "Epoch:  2 iteration:  34000 ,average train loss:  1.6228 , val loss:  1.3065\n",
            "Epoch:  2 iteration:  35000 ,average train loss:  1.6353 , val loss:  1.8271\n",
            "Epoch:  2 iteration:  36000 ,average train loss:  1.6754 , val loss:  1.7519\n",
            "Epoch:  2 iteration:  37000 ,average train loss:  1.6815 , val loss:  2.1422\n",
            "Epoch:  2 iteration:  38000 ,average train loss:  1.6929 , val loss:  1.7761\n",
            "Epoch:  2 iteration:  39000 ,average train loss:  1.6214 , val loss:  1.347\n",
            "Epoch:  2 iteration:  40000 ,average train loss:  1.6502 , val loss:  1.5982\n",
            "Epoch:  2 iteration:  41000 ,average train loss:  1.6488 , val loss:  1.626\n",
            "Epoch:  2 iteration:  42000 ,average train loss:  1.5934 , val loss:  2.0677\n",
            "Epoch:  2 iteration:  43000 ,average train loss:  1.6733 , val loss:  2.8071\n",
            "Epoch:  2 iteration:  44000 ,average train loss:  1.6369 , val loss:  1.279\n",
            "Epoch:  2 iteration:  45000 ,average train loss:  1.7253 , val loss:  1.6812\n",
            "Epoch:  2 iteration:  46000 ,average train loss:  1.6449 , val loss:  1.6899\n",
            "Epoch:  2 iteration:  47000 ,average train loss:  1.621 , val loss:  1.7522\n",
            "Epoch:  2 iteration:  48000 ,average train loss:  1.5811 , val loss:  1.7017\n",
            "Epoch:  2 iteration:  49000 ,average train loss:  1.5321 , val loss:  1.7505\n",
            "Epoch:  2 iteration:  50000 ,average train loss:  1.6564 , val loss:  1.7223\n",
            "Epoch:  2 iteration:  51000 ,average train loss:  1.7162 , val loss:  1.3801\n",
            "Epoch:  2 iteration:  52000 ,average train loss:  1.6376 , val loss:  1.7997\n",
            "Epoch:  2 iteration:  53000 ,average train loss:  1.6651 , val loss:  1.4897\n",
            "Epoch:  2 iteration:  54000 ,average train loss:  1.6269 , val loss:  1.8482\n",
            "Epoch:  2 iteration:  55000 ,average train loss:  1.621 , val loss:  1.7228\n",
            "Epoch:  2 iteration:  56000 ,average train loss:  1.6513 , val loss:  1.8266\n",
            "Epoch:  2 iteration:  57000 ,average train loss:  1.6472 , val loss:  1.2302\n",
            "Epoch:  2 iteration:  58000 ,average train loss:  1.5786 , val loss:  1.4694\n",
            "Epoch:  2 iteration:  59000 ,average train loss:  1.5705 , val loss:  2.2933\n",
            "Epoch:  2 iteration:  60000 ,average train loss:  1.5934 , val loss:  1.9039\n",
            "Epoch:  2 iteration:  61000 ,average train loss:  1.5639 , val loss:  1.3846\n",
            "Epoch:  2 iteration:  62000 ,average train loss:  1.6463 , val loss:  1.7385\n",
            "Epoch:  2 iteration:  63000 ,average train loss:  1.6943 , val loss:  1.6548\n",
            "Epoch:  2 iteration:  64000 ,average train loss:  1.6282 , val loss:  1.753\n",
            "Epoch:  2 iteration:  65000 ,average train loss:  1.6488 , val loss:  1.7709\n",
            "Epoch:  2 iteration:  66000 ,average train loss:  1.6468 , val loss:  2.0141\n",
            "Epoch:  2 iteration:  67000 ,average train loss:  1.6835 , val loss:  1.686\n",
            "Epoch:  2 iteration:  68000 ,average train loss:  1.6418 , val loss:  1.983\n",
            "Epoch:  2 iteration:  69000 ,average train loss:  1.7511 , val loss:  2.1001\n",
            "Epoch:  2 iteration:  70000 ,average train loss:  1.652 , val loss:  1.4728\n",
            "Epoch:  2 iteration:  71000 ,average train loss:  1.6145 , val loss:  2.0374\n",
            "Epoch:  2 iteration:  72000 ,average train loss:  1.6699 , val loss:  1.751\n",
            "Epoch:  2 iteration:  73000 ,average train loss:  1.6766 , val loss:  1.5735\n",
            "Epoch:  2 iteration:  74000 ,average train loss:  1.6053 , val loss:  1.7735\n",
            "Epoch:  2 iteration:  75000 ,average train loss:  1.5871 , val loss:  1.0744\n",
            "Epoch:  2 iteration:  76000 ,average train loss:  1.5749 , val loss:  1.793\n",
            "Epoch:  2 iteration:  77000 ,average train loss:  1.6472 , val loss:  1.3666\n",
            "Epoch:  2 iteration:  78000 ,average train loss:  1.6489 , val loss:  1.9757\n",
            "Epoch:  2 iteration:  79000 ,average train loss:  1.6051 , val loss:  2.1971\n",
            "Epoch:  2 iteration:  80000 ,average train loss:  1.662 , val loss:  2.5911\n",
            "Epoch:  2 iteration:  81000 ,average train loss:  1.642 , val loss:  1.6764\n",
            "Epoch:  2 iteration:  82000 ,average train loss:  1.6183 , val loss:  1.1508\n",
            "Epoch:  2 iteration:  83000 ,average train loss:  1.5747 , val loss:  1.558\n",
            "Epoch:  2 iteration:  84000 ,average train loss:  1.6138 , val loss:  1.8999\n",
            "Epoch:  2 iteration:  85000 ,average train loss:  1.6594 , val loss:  1.7534\n",
            "Epoch:  2 iteration:  86000 ,average train loss:  1.6292 , val loss:  1.5596\n",
            "Epoch:  2 iteration:  87000 ,average train loss:  1.673 , val loss:  1.8373\n",
            "Epoch:  2 iteration:  88000 ,average train loss:  1.6135 , val loss:  1.5324\n",
            "Epoch:  2 iteration:  89000 ,average train loss:  1.6306 , val loss:  0.9944\n",
            "Epoch:  2 iteration:  90000 ,average train loss:  1.6402 , val loss:  1.2654\n",
            "Epoch:  2 iteration:  91000 ,average train loss:  1.7402 , val loss:  1.7199\n",
            "Epoch:  2 iteration:  92000 ,average train loss:  1.6371 , val loss:  1.9805\n",
            "Epoch:  2 iteration:  93000 ,average train loss:  1.6718 , val loss:  1.9251\n",
            "Epoch:  2 iteration:  94000 ,average train loss:  1.6653 , val loss:  1.7066\n",
            "Epoch:  2 iteration:  95000 ,average train loss:  1.6008 , val loss:  1.6853\n",
            "Epoch:  2 iteration:  96000 ,average train loss:  1.6238 , val loss:  1.8976\n",
            "Epoch:  2 iteration:  97000 ,average train loss:  1.697 , val loss:  1.3878\n",
            "Epoch:  2 iteration:  98000 ,average train loss:  1.6279 , val loss:  1.4136\n",
            "Epoch:  2 iteration:  99000 ,average train loss:  1.568 , val loss:  1.924\n",
            "Epoch:  2 iteration:  100000 ,average train loss:  1.607 , val loss:  1.8133\n",
            "Epoch:  3 iteration:  1000 ,average train loss:  1.7289 , val loss:  1.5831\n",
            "Epoch:  3 iteration:  2000 ,average train loss:  1.635 , val loss:  2.3224\n",
            "Epoch:  3 iteration:  3000 ,average train loss:  1.6029 , val loss:  2.6974\n",
            "Epoch:  3 iteration:  4000 ,average train loss:  1.6506 , val loss:  1.1199\n",
            "Epoch:  3 iteration:  5000 ,average train loss:  1.6254 , val loss:  1.3653\n",
            "Epoch:  3 iteration:  6000 ,average train loss:  1.7022 , val loss:  1.7957\n",
            "Epoch:  3 iteration:  7000 ,average train loss:  1.6213 , val loss:  1.8117\n",
            "Epoch:  3 iteration:  8000 ,average train loss:  1.5691 , val loss:  2.2367\n",
            "Epoch:  3 iteration:  9000 ,average train loss:  1.6494 , val loss:  1.5575\n",
            "Epoch:  3 iteration:  10000 ,average train loss:  1.6769 , val loss:  1.6014\n",
            "Epoch:  3 iteration:  11000 ,average train loss:  1.6128 , val loss:  1.6785\n",
            "Epoch:  3 iteration:  12000 ,average train loss:  1.6198 , val loss:  1.8197\n",
            "Epoch:  3 iteration:  13000 ,average train loss:  1.675 , val loss:  2.123\n",
            "Epoch:  3 iteration:  14000 ,average train loss:  1.679 , val loss:  1.8897\n",
            "Epoch:  3 iteration:  15000 ,average train loss:  1.6335 , val loss:  1.6111\n",
            "Epoch:  3 iteration:  16000 ,average train loss:  1.7279 , val loss:  1.3336\n",
            "Epoch:  3 iteration:  17000 ,average train loss:  1.65 , val loss:  1.5519\n",
            "Epoch:  3 iteration:  18000 ,average train loss:  1.5983 , val loss:  1.8747\n",
            "Epoch:  3 iteration:  19000 ,average train loss:  1.6005 , val loss:  1.6744\n",
            "Epoch:  3 iteration:  20000 ,average train loss:  1.6025 , val loss:  1.0799\n",
            "Epoch:  3 iteration:  21000 ,average train loss:  1.6093 , val loss:  1.7789\n",
            "Epoch:  3 iteration:  22000 ,average train loss:  1.5976 , val loss:  1.45\n",
            "Epoch:  3 iteration:  23000 ,average train loss:  1.5711 , val loss:  1.6839\n",
            "Epoch:  3 iteration:  24000 ,average train loss:  1.6436 , val loss:  1.4045\n",
            "Epoch:  3 iteration:  25000 ,average train loss:  1.6204 , val loss:  1.5825\n",
            "Epoch:  3 iteration:  26000 ,average train loss:  1.6579 , val loss:  2.0013\n",
            "Epoch:  3 iteration:  27000 ,average train loss:  1.632 , val loss:  1.4645\n",
            "Epoch:  3 iteration:  28000 ,average train loss:  1.6089 , val loss:  1.8147\n",
            "Epoch:  3 iteration:  29000 ,average train loss:  1.5779 , val loss:  2.0543\n",
            "Epoch:  3 iteration:  30000 ,average train loss:  1.6571 , val loss:  1.6065\n",
            "Epoch:  3 iteration:  31000 ,average train loss:  1.6987 , val loss:  1.3359\n",
            "Epoch:  3 iteration:  32000 ,average train loss:  1.6514 , val loss:  1.9627\n",
            "Epoch:  3 iteration:  33000 ,average train loss:  1.616 , val loss:  1.334\n",
            "Epoch:  3 iteration:  34000 ,average train loss:  1.6302 , val loss:  1.3954\n",
            "Epoch:  3 iteration:  35000 ,average train loss:  1.6483 , val loss:  1.51\n",
            "Epoch:  3 iteration:  36000 ,average train loss:  1.6898 , val loss:  1.4372\n",
            "Epoch:  3 iteration:  37000 ,average train loss:  1.6843 , val loss:  1.5709\n",
            "Epoch:  3 iteration:  38000 ,average train loss:  1.7051 , val loss:  1.6037\n",
            "Epoch:  3 iteration:  39000 ,average train loss:  1.6314 , val loss:  1.9792\n",
            "Epoch:  3 iteration:  40000 ,average train loss:  1.6609 , val loss:  1.5581\n",
            "Epoch:  3 iteration:  41000 ,average train loss:  1.6653 , val loss:  1.6017\n",
            "Epoch:  3 iteration:  42000 ,average train loss:  1.6032 , val loss:  1.9785\n",
            "Epoch:  3 iteration:  43000 ,average train loss:  1.6755 , val loss:  1.9299\n",
            "Epoch:  3 iteration:  44000 ,average train loss:  1.6592 , val loss:  1.5452\n",
            "Epoch:  3 iteration:  45000 ,average train loss:  1.7381 , val loss:  1.8987\n",
            "Epoch:  3 iteration:  46000 ,average train loss:  1.6609 , val loss:  1.9117\n",
            "Epoch:  3 iteration:  47000 ,average train loss:  1.6337 , val loss:  1.2875\n",
            "Epoch:  3 iteration:  48000 ,average train loss:  1.598 , val loss:  2.6191\n",
            "Epoch:  3 iteration:  49000 ,average train loss:  1.5442 , val loss:  1.6614\n",
            "Epoch:  3 iteration:  50000 ,average train loss:  1.6713 , val loss:  1.7922\n",
            "Epoch:  3 iteration:  51000 ,average train loss:  1.7379 , val loss:  2.0334\n",
            "Epoch:  3 iteration:  52000 ,average train loss:  1.6567 , val loss:  1.6121\n",
            "Epoch:  3 iteration:  53000 ,average train loss:  1.6801 , val loss:  1.6825\n",
            "Epoch:  3 iteration:  54000 ,average train loss:  1.651 , val loss:  2.683\n",
            "Epoch:  3 iteration:  55000 ,average train loss:  1.632 , val loss:  1.9003\n",
            "Epoch:  3 iteration:  56000 ,average train loss:  1.6785 , val loss:  1.7215\n",
            "Epoch:  3 iteration:  57000 ,average train loss:  1.6616 , val loss:  2.1537\n",
            "Epoch:  3 iteration:  58000 ,average train loss:  1.6076 , val loss:  1.648\n",
            "Epoch:  3 iteration:  59000 ,average train loss:  1.5846 , val loss:  1.4074\n",
            "Epoch:  3 iteration:  60000 ,average train loss:  1.6135 , val loss:  1.6538\n",
            "Epoch:  3 iteration:  61000 ,average train loss:  1.5862 , val loss:  1.7402\n",
            "Epoch:  3 iteration:  62000 ,average train loss:  1.6621 , val loss:  2.3039\n",
            "Epoch:  3 iteration:  63000 ,average train loss:  1.7088 , val loss:  1.4541\n",
            "Epoch:  3 iteration:  64000 ,average train loss:  1.6457 , val loss:  2.0251\n",
            "Epoch:  3 iteration:  65000 ,average train loss:  1.661 , val loss:  1.7855\n",
            "Epoch:  3 iteration:  66000 ,average train loss:  1.6821 , val loss:  1.9368\n",
            "Epoch:  3 iteration:  67000 ,average train loss:  1.7023 , val loss:  1.7695\n",
            "Epoch:  3 iteration:  68000 ,average train loss:  1.6572 , val loss:  1.512\n",
            "Epoch:  3 iteration:  69000 ,average train loss:  1.7712 , val loss:  1.8769\n",
            "Epoch:  3 iteration:  70000 ,average train loss:  1.6748 , val loss:  1.6615\n",
            "Epoch:  3 iteration:  71000 ,average train loss:  1.6354 , val loss:  1.6858\n",
            "Epoch:  3 iteration:  72000 ,average train loss:  1.693 , val loss:  2.2513\n",
            "Epoch:  3 iteration:  73000 ,average train loss:  1.7009 , val loss:  2.739\n",
            "Epoch:  3 iteration:  74000 ,average train loss:  1.627 , val loss:  1.323\n",
            "Epoch:  3 iteration:  75000 ,average train loss:  1.6153 , val loss:  1.7875\n",
            "Epoch:  3 iteration:  76000 ,average train loss:  1.6036 , val loss:  1.1657\n",
            "Epoch:  3 iteration:  77000 ,average train loss:  1.6744 , val loss:  1.0731\n",
            "Epoch:  3 iteration:  78000 ,average train loss:  1.6718 , val loss:  1.9778\n",
            "Epoch:  3 iteration:  79000 ,average train loss:  1.6361 , val loss:  1.6625\n",
            "Epoch:  3 iteration:  80000 ,average train loss:  1.691 , val loss:  1.9516\n",
            "Epoch:  3 iteration:  81000 ,average train loss:  1.6762 , val loss:  1.6646\n",
            "Epoch:  3 iteration:  82000 ,average train loss:  1.6454 , val loss:  2.3901\n",
            "Epoch:  3 iteration:  83000 ,average train loss:  1.6017 , val loss:  2.1271\n",
            "Epoch:  3 iteration:  84000 ,average train loss:  1.6423 , val loss:  2.08\n",
            "Epoch:  3 iteration:  85000 ,average train loss:  1.6823 , val loss:  2.0506\n",
            "Epoch:  3 iteration:  86000 ,average train loss:  1.6619 , val loss:  2.4352\n",
            "Epoch:  3 iteration:  87000 ,average train loss:  1.6907 , val loss:  2.1631\n",
            "Epoch:  3 iteration:  88000 ,average train loss:  1.6359 , val loss:  1.7323\n",
            "Epoch:  3 iteration:  89000 ,average train loss:  1.6468 , val loss:  1.9362\n",
            "Epoch:  3 iteration:  90000 ,average train loss:  1.651 , val loss:  2.1231\n",
            "Epoch:  3 iteration:  91000 ,average train loss:  1.7521 , val loss:  2.1843\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b59a1cc8738e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "hidden = torch.zeros(num_layers, batch_size, hidden_dim, requires_grad=True).to(device)\n",
        "j = 0\n",
        "\n",
        "for epoch in range(1,epochs+1):\n",
        "  train_loss = 0\n",
        "  for i in range(train_batch_num):\n",
        "    net.train()\n",
        "    optimizer.zero_grad()\n",
        "    x, y = batch_generator(i, X_train, y_train)\n",
        "    output, hidden = net(x,hidden)\n",
        "    loss = criterion(output, y)\n",
        "    hidden = hidden.detach()\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(parameters=net.parameters(), max_norm = 10)\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "    # train_loss_list.append(loss.item())\n",
        "\n",
        "    if i%1000 == 0 and i != 0:\n",
        "      train_loss_list.append(train_loss/1000)\n",
        "      net.eval()     \n",
        "      #validation\n",
        "      val_loss = 0\n",
        "      x, y = batch_generator(j%val_batch_num, X_val, y_val)\n",
        "      # x, y = batch_generator(100, X_val, y_val)\n",
        "      output, _ = net(x, hidden)\n",
        "      output = output.to(device)\n",
        "      loss = criterion(output, y)\n",
        "      print('Epoch: ',epoch,'iteration: ',i,',average train loss: ',round(train_loss/1000,4),\n",
        "              ', val loss: ',round(loss.item(),4))\n",
        "\n",
        "      train_loss = 0\n",
        "      j += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIw1a_bk2u0q"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(7,7))\n",
        "ax1 = fig.add_subplot(111)\n",
        "ax1.plot(range(len(train_loss_list)),train_loss_list,label='train loss')\n",
        "ax1.legend()\n",
        "ax1.set_title('Loss Plot')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNUj7L8bwRHn"
      },
      "source": [
        "Produce new sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l45CvyCOLD2o"
      },
      "outputs": [],
      "source": [
        "i = int(val_batch_num*np.random.random(1))\n",
        "\n",
        "hidden = torch.zeros(num_layers, batch_size, hidden_dim, requires_grad=True).to(device)\n",
        "x, y = batch_generator(i, X_val, y_val)\n",
        "output, hidden = net(x, hidden)\n",
        "\n",
        "train=[]\n",
        "for id in y:\n",
        "  train.append(char_index[int(id)])\n",
        "\n",
        "predict=[]\n",
        "for id in torch.argmax(output,axis=1):\n",
        "    predict.append(char_index[int(id)])\n",
        "''.join(train), ''.join(predict)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}